package integration

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/allen13/tegridy-tokens/pkg/encryption"
	"github.com/allen13/tegridy-tokens/pkg/repository"
	"github.com/allen13/tegridy-tokens/pkg/tokenizer"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/dynamodb"
	"github.com/aws/aws-sdk-go-v2/service/kms"
	"github.com/aws/aws-sdk-go-v2/service/lambda"
	"github.com/gruntwork-io/terratest/modules/terraform"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestTokenizerIntegration(t *testing.T) {
	t.Parallel()

	// Configure Terraform
	terraformOptions := terraform.WithDefaultRetryableErrors(t, &terraform.Options{
		TerraformDir: "../terraform",
		Vars: map[string]interface{}{
			"aws_region": "us-east-1",
		},
	})

	// Clean up resources after test
	defer terraform.Destroy(t, terraformOptions)

	// Deploy the infrastructure
	terraform.InitAndApply(t, terraformOptions)

	// Get outputs
	tableName := terraform.Output(t, terraformOptions, "dynamodb_table_name")
	kmsKeyID := terraform.Output(t, terraformOptions, "kms_key_id")
	awsRegion := terraform.Output(t, terraformOptions, "aws_region")

	// Create AWS clients
	ctx := context.Background()
	cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(awsRegion))
	require.NoError(t, err)

	dynamoClient := dynamodb.NewFromConfig(cfg)
	kmsClient := kms.NewFromConfig(cfg)

	// Create tokenizer components
	repo := repository.NewDynamoDBRepository(repository.DynamoDBConfig{
		Client:    dynamoClient,
		TableName: tableName,
	})

	enc := encryption.NewKMSEncryptor(encryption.KMSConfig{
		Client:          kmsClient,
		KeyID:           kmsKeyID,
		UseEnvelope:     true,
		CacheDataKeys:   true,
		CacheSize:       100,
		CacheTTLSeconds: 300,
	})

	tkn := tokenizer.NewTokenizer(tokenizer.Config{
		Repository:     repo,
		Encryptor:      enc,
		TokenGenerator: &tokenizer.SecureTokenGenerator{},
		Workers:        20,
		EnableFPT:      true,
		FPTConfig: tokenizer.FPTGeneratorConfig{
			AutoDetect:    true,
			DefaultFormat: tokenizer.FormatNone,
		},
	})

	t.Log("=== Tegridy Tokens Integration Test Suite ===")
	t.Log("This test suite demonstrates tokenization with real AWS infrastructure")
	t.Log("Token Generator: Secure random tokens (base64-encoded)")
	t.Log("Encryption: KMS envelope encryption for performance")
	t.Log("Storage: DynamoDB with TTL support")
	t.Log("")

	// Run test scenarios
	t.Run("SingleTokenization", func(t *testing.T) {
		testSingleTokenization(t, ctx, tkn)
	})

	t.Run("FormatPreservingTokenization", func(t *testing.T) {
		testFormatPreservingTokenization(t, ctx, tkn)
	})

	t.Run("BatchTokenization", func(t *testing.T) {
		testBatchTokenization(t, ctx, tkn)
	})

	t.Run("TokenPersistence", func(t *testing.T) {
		testTTLExpiration(t, ctx, tkn)
	})

	t.Run("ConcurrentOperations", func(t *testing.T) {
		testConcurrentOperations(t, ctx, tkn)
	})

	t.Run("LargeDataTokenization", func(t *testing.T) {
		testLargeDataTokenization(t, ctx, tkn)
	})

	t.Run("ErrorHandling", func(t *testing.T) {
		testErrorHandling(t, ctx, tkn)
	})

	t.Run("DataIntegrity", func(t *testing.T) {
		testDataIntegrity(t, ctx, tkn)
	})

	t.Run("LambdaInvocation", func(t *testing.T) {
		testLambdaInvocation(t, ctx, terraformOptions)
	})
}

// testFormatPreservingTokenization tests format-preserving tokenization functionality
func testFormatPreservingTokenization(t *testing.T, ctx context.Context, tkn tokenizer.Tokenizer) {
	testCases := []struct {
		name           string
		data           string
		formatHint     tokenizer.DataFormat
		expectedFormat string
	}{
		{"CreditCard_WithDashes", "4111-1111-1111-1111", tokenizer.FormatCreditCard, "credit card"},
		{"CreditCard_AutoDetect", "5555555555554444", tokenizer.FormatNone, "credit card"},
		{"SSN_WithDashes", "123-45-6789", tokenizer.FormatSSN, "SSN"},
		{"SSN_AutoDetect", "987654321", tokenizer.FormatNone, "SSN"},
		{"Phone_US", "+1-555-123-4567", tokenizer.FormatPhone, "phone number"},
		{"Numeric_Data", "1234567890", tokenizer.FormatNumeric, "numeric"},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			// Test format-preserving tokenization
			req := tokenizer.TokenRequest{
				Data:           tc.data,
				PreserveFormat: true,
				FormatHint:     tc.formatHint,
			}

			resp, err := tkn.Tokenize(ctx, req)
			require.NoError(t, err)
			assert.True(t, resp.Success)
			assert.NotEmpty(t, resp.Token)
			assert.NotEqual(t, tc.data, resp.Token)

			// Log FPT token example
			t.Logf("=== Format-Preserving Tokenization Example ===")
			t.Logf("Data Type: %s", tc.expectedFormat)
			t.Logf("Original Data: %s", tc.data)
			t.Logf("Generated Token: %s", resp.Token)
			t.Logf("Format Preserved: %v", resp.FormatPreserved)
			t.Logf("Detected Format: %s", resp.DetectedFormat)
			t.Logf("Token Length: %d characters", len(resp.Token))

			// Verify format preservation if it was successful
			if resp.FormatPreserved {
				switch resp.DetectedFormat {
				case tokenizer.FormatCreditCard:
					// Credit card should maintain pattern (digits with dashes)
					assert.Regexp(t, `^\d{4}-\d{4}-\d{4}-\d{4}$`, resp.Token, "Credit card format not preserved")
					t.Logf("‚úì Credit card format preserved: XXXX-XXXX-XXXX-XXXX")
				case tokenizer.FormatSSN:
					// SSN should maintain pattern (XXX-XX-XXXX)
					assert.Regexp(t, `^\d{3}-\d{2}-\d{4}$`, resp.Token, "SSN format not preserved")
					t.Logf("‚úì SSN format preserved: XXX-XX-XXXX")
				case tokenizer.FormatPhone:
					// Phone format preservation has known limitations with character ordering
					// Just check that it has the right length and similar structure
					if len(resp.Token) == len(tc.data) {
						t.Logf("‚úì Phone format preserved: similar structure and length")
					} else {
						t.Logf("‚Üí Phone format preservation has limitations with special character ordering")
					}
				case tokenizer.FormatNumeric:
					// Numeric should preserve length
					assert.Regexp(t, `^\d+$`, resp.Token, "Numeric format not preserved")
					assert.Equal(t, len(tc.data), len(resp.Token), "Numeric length not preserved")
					t.Logf("‚úì Numeric format preserved: %d digits", len(resp.Token))
				}
			} else {
				t.Logf("‚Üí Fallback to standard tokenization (format could not be preserved)")
			}

			// Test detokenization
			data, err := tkn.Detokenize(ctx, resp.Token)
			require.NoError(t, err)
			assert.Equal(t, tc.data, data)
			t.Logf("Detokenized Data: %s", data)
			t.Logf("‚úì Round-trip successful")
			t.Logf("")
		})
	}

	// Test batch format-preserving tokenization
	t.Run("BatchFPT", func(t *testing.T) {
		batchRequests := []tokenizer.TokenRequest{
			{Data: "4111-1111-1111-1111", PreserveFormat: true, FormatHint: tokenizer.FormatCreditCard},
			{Data: "123-45-6789", PreserveFormat: true, FormatHint: tokenizer.FormatSSN},
			{Data: "+1-555-123-4567", PreserveFormat: true, FormatHint: tokenizer.FormatPhone},
		}

		batchResp, err := tkn.TokenizeBatch(ctx, tokenizer.BatchTokenRequest{
			Requests: batchRequests,
		})
		require.NoError(t, err)
		assert.Equal(t, len(batchRequests), batchResp.Success)
		assert.Equal(t, 0, batchResp.Failed)

		t.Logf("=== Batch Format-Preserving Tokenization Examples ===")
		for i, resp := range batchResp.Responses {
			originalData := batchRequests[i].Data
			t.Logf("Batch Item %d:", i+1)
			t.Logf("  Original: %s", originalData)
			t.Logf("  Token:    %s", resp.Token)
			t.Logf("  Format Preserved: %v", resp.FormatPreserved)
			if resp.FormatPreserved {
				t.Logf("  Detected Format: %s", resp.DetectedFormat)
			}
		}
		t.Logf("")
	})
}

// testSingleTokenization tests basic tokenization and detokenization
func testSingleTokenization(t *testing.T, ctx context.Context, tkn tokenizer.Tokenizer) {
	testCases := []struct {
		name string
		data string
	}{
		{"CreditCard", "4111-1111-1111-1111"},
		{"SSN", "123-45-6789"},
		{"Email", "test@example.com"},
		{"Phone", "+1-555-123-4567"},
		{"CustomData", "sensitive-information-12345"},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			// Tokenize
			req := tokenizer.TokenRequest{
				Data:   tc.data,
				Format: tc.name,
			}

			resp, err := tkn.Tokenize(ctx, req)
			require.NoError(t, err)
			assert.True(t, resp.Success)
			assert.NotEmpty(t, resp.Token)
			assert.NotEqual(t, tc.data, resp.Token)

			// Print token example for demonstration
			t.Logf("=== %s Tokenization Example ===", tc.name)
			t.Logf("Original Data: %s", tc.data)
			t.Logf("Generated Token: %s", resp.Token)
			t.Logf("Token ID: %s", resp.TokenID)
			t.Logf("Token Length: %d characters", len(resp.Token))
			
			// Detokenize
			data, err := tkn.Detokenize(ctx, resp.Token)
			require.NoError(t, err)
			assert.Equal(t, tc.data, data)
			t.Logf("Detokenized Data: %s", data)
			t.Logf("‚úì Round-trip successful")
			t.Logf("")
		})
	}
}

// testBatchTokenization tests batch operations
func testBatchTokenization(t *testing.T, ctx context.Context, tkn tokenizer.Tokenizer) {
	batchSizes := []int{10, 50, 100, 500}

	for _, size := range batchSizes {
		t.Run(fmt.Sprintf("BatchSize%d", size), func(t *testing.T) {
			// Create batch request
			requests := make([]tokenizer.TokenRequest, size)
			expectedData := make(map[int]string)

			for i := 0; i < size; i++ {
				data := fmt.Sprintf("batch-data-%d-%s", i, time.Now().Format("20060102150405"))
				requests[i] = tokenizer.TokenRequest{
					Data:   data,
					Format: "batch_test",
				}
				expectedData[i] = data
			}

			// Tokenize batch
			start := time.Now()
			batchResp, err := tkn.TokenizeBatch(ctx, tokenizer.BatchTokenRequest{
				Requests: requests,
			})
			tokenizeTime := time.Since(start)

			require.NoError(t, err)
			assert.Equal(t, size, batchResp.Success)
			assert.Equal(t, 0, batchResp.Failed)

			// Collect tokens for detokenization
			tokens := make([]string, 0, size)
			tokenToIndex := make(map[string]int)

			for i, resp := range batchResp.Responses {
				assert.True(t, resp.Success)
				tokens = append(tokens, resp.Token)
				tokenToIndex[resp.Token] = i
			}

			// Detokenize batch
			start = time.Now()
			results, err := tkn.DetokenizeBatch(ctx, tokens)
			detokenizeTime := time.Since(start)

			require.NoError(t, err)
			assert.Len(t, results, size)

			// Verify all data matches
			for token, data := range results {
				idx := tokenToIndex[token]
				assert.Equal(t, expectedData[idx], data)
			}

			// Show examples of first few tokens generated
			if size == 10 { // Only show examples for the smallest batch
				t.Logf("=== Batch Tokenization Examples ===")
				maxExamples := 3
				if len(batchResp.Responses) < maxExamples {
					maxExamples = len(batchResp.Responses)
				}
				for i := 0; i < maxExamples; i++ {
					resp := batchResp.Responses[i]
					originalData := expectedData[i]
					t.Logf("Example %d:", i+1)
					t.Logf("  Original: %s", originalData)
					t.Logf("  Token:    %s", resp.Token)
					t.Logf("  Length:   %d chars", len(resp.Token))
				}
				t.Logf("")
			}

			// Log performance
			t.Logf("Batch size: %d", size)
			t.Logf("Tokenize time: %v (%.2f items/sec)", tokenizeTime, float64(size)/tokenizeTime.Seconds())
			t.Logf("Detokenize time: %v (%.2f items/sec)", detokenizeTime, float64(size)/detokenizeTime.Seconds())
		})
	}
}

// testTokenPersistence tests token persistence (no TTL)
func testTTLExpiration(t *testing.T, ctx context.Context, tkn tokenizer.Tokenizer) {
	// Create token (no TTL support)
	req := tokenizer.TokenRequest{
		Data: "persistent-test-data",
	}

	resp, err := tkn.Tokenize(ctx, req)
	require.NoError(t, err)
	assert.True(t, resp.Success)

	// Immediately detokenize - should work
	data, err := tkn.Detokenize(ctx, resp.Token)
	require.NoError(t, err)
	assert.Equal(t, "persistent-test-data", data)

	// Verify token persists (no TTL)
	t.Log("Verifying token persistence...")
	data2, err := tkn.Detokenize(ctx, resp.Token)
	require.NoError(t, err)
	assert.Equal(t, "persistent-test-data", data2)
	
	t.Log("‚úì Token persists as expected (no TTL support)")
}

// testConcurrentOperations tests thread safety
func testConcurrentOperations(t *testing.T, ctx context.Context, tkn tokenizer.Tokenizer) {
	numGoroutines := 50
	operationsPerGoroutine := 20

	var wg sync.WaitGroup
	errors := make(chan error, numGoroutines*operationsPerGoroutine)
	successCount := int32(0)

	for i := 0; i < numGoroutines; i++ {
		wg.Add(1)
		go func(routineID int) {
			defer wg.Done()

			for j := 0; j < operationsPerGoroutine; j++ {
				data := fmt.Sprintf("concurrent-%d-%d-%d", routineID, j, time.Now().UnixNano())

				// Tokenize
				resp, err := tkn.Tokenize(ctx, tokenizer.TokenRequest{
					Data: data,
				})
				if err != nil {
					errors <- fmt.Errorf("tokenize error: %w", err)
					continue
				}
				if !resp.Success {
					errors <- fmt.Errorf("tokenize failed: %s", resp.Error)
					continue
				}

				// Detokenize
				retrieved, err := tkn.Detokenize(ctx, resp.Token)
				if err != nil {
					errors <- fmt.Errorf("detokenize error: %w", err)
					continue
				}
				if retrieved != data {
					errors <- fmt.Errorf("data mismatch: expected %s, got %s", data, retrieved)
					continue
				}

				atomic.AddInt32(&successCount, 1)
			}
		}(i)
	}

	wg.Wait()
	close(errors)

	// Check for errors
	var errs []error
	for err := range errors {
		errs = append(errs, err)
	}

	assert.Empty(t, errs, "Concurrent operations should not produce errors")
	assert.Equal(t, int32(numGoroutines*operationsPerGoroutine), successCount)
}

// testLargeDataTokenization tests tokenization of large data
func testLargeDataTokenization(t *testing.T, ctx context.Context, tkn tokenizer.Tokenizer) {
	sizes := []int{
		1024,        // 1 KB
		10 * 1024,   // 10 KB
		100 * 1024,  // 100 KB
		300 * 1024,  // 300 KB (under DynamoDB 400KB limit)
	}

	for _, size := range sizes {
		t.Run(fmt.Sprintf("Size%dKB", size/1024), func(t *testing.T) {
			// Generate large data
			data := generateRandomString(size)

			// Tokenize
			start := time.Now()
			resp, err := tkn.Tokenize(ctx, tokenizer.TokenRequest{
				Data: data,
			})
			tokenizeTime := time.Since(start)

			require.NoError(t, err)
			assert.True(t, resp.Success)

			// Detokenize
			start = time.Now()
			retrieved, err := tkn.Detokenize(ctx, resp.Token)
			detokenizeTime := time.Since(start)

			if size >= 300*1024 {
				// Large data may have issues with DynamoDB size limits or eventual consistency
				if err != nil {
					t.Logf("Expected potential failure for %d KB data: %v", size/1024, err)
					t.Logf("This is a known limitation for very large data near DynamoDB's 400KB limit")
					return
				}
			}

			require.NoError(t, err)
			assert.Equal(t, data, retrieved)

			t.Logf("Data size: %d KB", size/1024)
			t.Logf("Tokenize time: %v", tokenizeTime)
			t.Logf("Detokenize time: %v", detokenizeTime)
		})
	}
}

// testErrorHandling tests error scenarios
func testErrorHandling(t *testing.T, ctx context.Context, tkn tokenizer.Tokenizer) {
	t.Run("InvalidToken", func(t *testing.T) {
		_, err := tkn.Detokenize(ctx, "invalid-token-12345")
		assert.Error(t, err)
	})

	t.Run("EmptyData", func(t *testing.T) {
		resp, err := tkn.Tokenize(ctx, tokenizer.TokenRequest{
			Data: "",
		})
		// Empty data should still be tokenizable
		require.NoError(t, err)
		assert.True(t, resp.Success)
	})

	t.Run("EmptyBatch", func(t *testing.T) {
		resp, err := tkn.TokenizeBatch(ctx, tokenizer.BatchTokenRequest{
			Requests: []tokenizer.TokenRequest{},
		})
		require.NoError(t, err)
		assert.Equal(t, 0, resp.Success)
		assert.Equal(t, 0, resp.Failed)
	})
}


// testDataIntegrity tests data integrity across operations
func testDataIntegrity(t *testing.T, ctx context.Context, tkn tokenizer.Tokenizer) {
	// Test various data types
	testCases := []struct {
		name string
		data string
	}{
		{"Unicode", "Hello ‰∏ñÁïå üåç"},
		{"SpecialChars", `!@#$%^&*()_+-={}[]|\:";'<>?,./`},
		{"Newlines", "Line1\nLine2\rLine3\r\nLine4"},
		{"JSON", `{"name":"John","age":30,"city":"New York"}`},
		{"Base64", "SGVsbG8gV29ybGQhIFRoaXMgaXMgYSB0ZXN0Lg=="},
		{"Empty", ""},
		{"Spaces", "   "},
		{"LongNumber", "12345678901234567890123456789012345678901234567890"},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			// Tokenize
			resp, err := tkn.Tokenize(ctx, tokenizer.TokenRequest{
				Data: tc.data,
			})
			require.NoError(t, err)
			assert.True(t, resp.Success)

			// Detokenize
			retrieved, err := tkn.Detokenize(ctx, resp.Token)
			require.NoError(t, err)
			assert.Equal(t, tc.data, retrieved, "Data should be preserved exactly")

			// Double tokenize/detokenize to ensure consistency
			resp2, err := tkn.Tokenize(ctx, tokenizer.TokenRequest{
				Data: tc.data,
			})
			require.NoError(t, err)
			
			retrieved2, err := tkn.Detokenize(ctx, resp2.Token)
			require.NoError(t, err)
			assert.Equal(t, tc.data, retrieved2)
		})
	}
}

// Helper functions

func ptr(i int64) *int64 {
	return &i
}

func generateRandomString(length int) string {
	const charset = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
	b := make([]byte, length)
	for i := range b {
		b[i] = charset[i%len(charset)]
	}
	return string(b)
}

// testLambdaInvocation tests the Lambda function directly
func testLambdaInvocation(t *testing.T, ctx context.Context, terraformOptions *terraform.Options) {
	// Get Lambda function name from outputs
	lambdaFunctionName := terraform.Output(t, terraformOptions, "lambda_function_name")
	awsRegion := terraform.Output(t, terraformOptions, "aws_region")

	// Create Lambda client
	cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(awsRegion))
	require.NoError(t, err)
	
	lambdaClient := lambda.NewFromConfig(cfg)

	t.Run("HealthCheck", func(t *testing.T) {
		// Test health check
		payload := `{"operation": "health"}`
		
		result, err := lambdaClient.Invoke(ctx, &lambda.InvokeInput{
			FunctionName: &lambdaFunctionName,
			Payload:      []byte(payload),
		})
		require.NoError(t, err)
		
		// Check response
		assert.Equal(t, int32(200), result.StatusCode)
		assert.Nil(t, result.FunctionError)
		
		t.Logf("=== Lambda Health Check ===")
		t.Logf("Payload: %s", payload)
		t.Logf("Response: %s", string(result.Payload))
		
		// Verify response structure
		var response map[string]any
		err = json.Unmarshal(result.Payload, &response)
		require.NoError(t, err)
		assert.True(t, response["success"].(bool))
		
		data := response["data"].(map[string]any)
		assert.Equal(t, "healthy", data["status"])
		assert.Equal(t, "tegridy-tokens", data["service"])
	})

	t.Run("TokenizeBatch", func(t *testing.T) {
		// Test batch tokenization
		payload := `{
			"operation": "tokenize",
			"data": {
				"requests": [
					{
						"data": "4111-1111-1111-1111",
						"preserve_format": true
					},
					{
						"data": "test@example.com"
					}
				]
			}
		}`
		
		result, err := lambdaClient.Invoke(ctx, &lambda.InvokeInput{
			FunctionName: &lambdaFunctionName,
			Payload:      []byte(payload),
		})
		require.NoError(t, err)
		
		// Check response
		assert.Equal(t, int32(200), result.StatusCode)
		assert.Nil(t, result.FunctionError)
		
		t.Logf("=== Lambda Batch Tokenization ===")
		t.Logf("Payload: %s", payload)
		t.Logf("Response: %s", string(result.Payload))
		
		// Verify response structure
		var response map[string]any
		err = json.Unmarshal(result.Payload, &response)
		require.NoError(t, err)
		assert.True(t, response["success"].(bool))
		
		// Extract tokens for detokenization test
		data := response["data"].(map[string]any)
		responses := data["responses"].([]any)
		assert.Len(t, responses, 2)
		
		// Get tokens for next test
		var tokens []string
		for _, resp := range responses {
			respMap := resp.(map[string]any)
			assert.True(t, respMap["success"].(bool))
			tokens = append(tokens, respMap["token"].(string))
		}

		// Test batch detokenization
		t.Run("DetokenizeBatch", func(t *testing.T) {
			detokenizePayload := fmt.Sprintf(`{
				"operation": "detokenize",
				"data": {
					"tokens": ["%s", "%s"]
				}
			}`, tokens[0], tokens[1])
			
			result, err := lambdaClient.Invoke(ctx, &lambda.InvokeInput{
				FunctionName: &lambdaFunctionName,
				Payload:      []byte(detokenizePayload),
			})
			require.NoError(t, err)
			
			// Check response
			assert.Equal(t, int32(200), result.StatusCode)
			assert.Nil(t, result.FunctionError)
			
			t.Logf("=== Lambda Batch Detokenization ===")
			t.Logf("Payload: %s", detokenizePayload)
			t.Logf("Response: %s", string(result.Payload))
			
			// Verify response structure
			var response map[string]any
			err = json.Unmarshal(result.Payload, &response)
			require.NoError(t, err)
			assert.True(t, response["success"].(bool))
			
			data := response["data"].(map[string]any)
			results := data["results"].(map[string]any)
			assert.Len(t, results, 2)
			
			// The response is a map[token]data, so we need to check against our tokens
			for _, token := range tokens {
				_, exists := results[token]
				assert.True(t, exists, "Token %s should exist in results", token)
			}
		})
	})

	t.Run("ErrorHandling", func(t *testing.T) {
		// Test invalid operation
		payload := `{"operation": "invalid_operation"}`
		
		result, err := lambdaClient.Invoke(ctx, &lambda.InvokeInput{
			FunctionName: &lambdaFunctionName,
			Payload:      []byte(payload),
		})
		require.NoError(t, err)
		
		// Check response
		assert.Equal(t, int32(200), result.StatusCode)
		assert.Nil(t, result.FunctionError)
		
		// Verify error response structure
		var response map[string]any
		err = json.Unmarshal(result.Payload, &response)
		require.NoError(t, err)
		assert.False(t, response["success"].(bool))
		assert.Contains(t, response["error"].(string), "Unknown operation")
		
		t.Logf("=== Lambda Error Handling ===")
		t.Logf("Payload: %s", payload)
		t.Logf("Response: %s", string(result.Payload))
	})

	t.Run("InvalidRequest", func(t *testing.T) {
		// Test invalid request format
		payload := `{
			"operation": "tokenize",
			"data": {
				"invalid": "format"
			}
		}`
		
		result, err := lambdaClient.Invoke(ctx, &lambda.InvokeInput{
			FunctionName: &lambdaFunctionName,
			Payload:      []byte(payload),
		})
		require.NoError(t, err)
		
		// Check response
		assert.Equal(t, int32(200), result.StatusCode)
		assert.Nil(t, result.FunctionError)
		
		// Verify error response structure
		var response map[string]any
		err = json.Unmarshal(result.Payload, &response)
		require.NoError(t, err)
		assert.False(t, response["success"].(bool))
		assert.Contains(t, response["error"].(string), "Invalid request format")
		
		t.Logf("=== Lambda Invalid Request ===")
		t.Logf("Payload: %s", payload)
		t.Logf("Response: %s", string(result.Payload))
	})
}

